---
title: 'CKME136 Capstone Project'
author: "Erma Fernando (Student Number 500925471)"
output:
  word_document: default
  pdf_document: default
  
---

### 1. Read the csv files in the folder. 
```{r}
#data2011<-read.csv(file="C:/RY/CKME136/OntarioGHG data/ModifiedData/bps_raw_2011_modified.csv",header=T,sep=",")

data2011<-read.csv(file="C:/RY/CKME136/OntarioGHG data/FinalData/bps_raw_2011_modified_20181105.csv",header=T,sep=",")

#data2012<-read.csv(file="C:/RY/CKME136/OntarioGHG #data/FinalData/bps_raw_2011_modified_20181105.csv",header=T,sep=",")
#data2013<-read.csv(file="C:/RY/CKME136/OntarioGHG #data/FinalData/bps_raw_2011_modified_20181105.csv",header=T,sep=",")
#data2014<-read.csv(file="C:/RY/CKME136/OntarioGHG #data/FinalData/bps_raw_2011_modified_20181105.csv",header=T,sep=",")
#data2015<-read.csv(file="C:/RY/CKME136/OntarioGHG #data/FinalData/bps_raw_2011_modified_20181105.csv",header=T,sep=",")
```

### 2. Merge the data frames for 2011, 2012, 2013, 1014, 2015
# all_data <-(rbind(data2011, data2012)
# all_data <-(rbind(all_data, data2013)
# all_data <-(rbind(all_data, data2014)
# all_data <-(rbind(all_data, data2015)
```{r}


```

### 3. Check the datatypes of the attributes. 
```{r}
str(data2011)
head(data2011)
tail(data2011)
#statistical info
summary(data2011)

# List rows of data that have missing values
sapply(data2011, function(x) sum(is.na(x)))

# missing data
data2011[!complete.cases(data2011),]

# count of various data types
table(unlist(lapply(data2011,class)))

#list the unique Operation types
sapply(data2011, function(x) unique(data2011$Operation.Type))


 ## load the tidyverse packages, incl. dplyr
install.packages("tidyverse")
library("tidyverse")  
library("dplyr")

 ## load the MASS package
#install.packages("MASS")
#library("MASS")

#create table of counts of records for each operation type arrange by desc counts

data2011 %>%
group_by(Operation.Type) %>%
summarise(freq=n(), totalGHG=sum(GHG.Emissions.Kg.)) %>%
arrange(desc(freq))

#create table of counts of records for each operation type arrange by desc ghg emisssion

data2011 %>%
group_by(Operation.Type) %>%
summarise(freq=n(), totalGHG=sum(GHG.Emissions.Kg.)) %>%
arrange(desc(totalGHG))

#create table of distinct operation types

data2011 %>%
distinct(Operation.Type)

# confirming data2011 is a dataframe

class(data2011)

# separate the data into 2 depending on whether energy intensity is based on volume or area

#separate data into dataframe having energy intensity based on area

data2011_area <-select(data2011, Year, Operation.Type, Floor.Area.Sq.ft., Electricity_kwh, Natural.Gas.Cubic.meter., Energy.Intensity..ekWh.sqft., GHG.Emissions.Kg.) 
  filter(data2011,Floor.Area.Sq.ft.> 0)
  
#separate data into dataframe having energy intensity based on volume - for sewer and water operation types

   data2011_volume<-select(data2011, Year, Operation.Type, Floor.Area.Sq.ft., Electricity_kwh, Natural.Gas.Cubic.meter., Energy.Intensity..ekWh.sqft., GHG.Emissions.Kg.) 
  filter(data2011,Floor.Area.Sq.ft.<= 0) 
  

## get numeric columns from data2011_area dataframe for normalization
 
xx_area<-select(data2011 , Floor.Area.Sq.ft., Electricity_kwh, Natural.Gas.Cubic.meter., Energy.Intensity..ekWh.sqft., GHG.Emissions.Kg.) 
  filter(data2011,Floor.Area.Sq.ft.<= 0) 
  
str(xx_area)



```

### 4. Correlation between the numerical attributes in xx_area.  Plot the data
```{r}
correlations <-cor(xx_area, method = "spearman", use = "pairwise.complete.obs") 
correlations
summary(correlations)

install.packages("reshape2")
install.packages("ggplot2")
library(ggplot2)
library(reshape2)

ggplot2.boxplot(data=xx_area)


ggplot(xx_area, aes(x = variable, y = value)) + 
    geom_boxplot()
#boxplot(xx_area)
boxplot(t(xx_area))
#hist(xx_area$g)
plot(x=data2011$Electricity, y=data2011$GHG.Emissions.Kg.)
mean_ghg<-mean(data2011_area$GHG.Emissions.Kg.)
max_ghg <-max(data2011_area$GHG.Emissions.Kg.)
min_ghg<-min(data2011_area$GHG.Emissions.Kg.)
sd_ghg<-sd(data2011_area$GHG.Emissions.Kg.)
mean_ghg
min_ghg
max_ghg
sd_ghg

library(corrplot)
corrplot(correlations, method="circle")

#table(data2011_area$Energy.Intensity..GJ.m2.)

```

### 5. Normalize
```

normalize <- function(x){
  return ((x-min(x))/(max(x) - min(x)))
}


xx_area_n <-as.data.frame(lapply(xx_area, normalize))
NROW(data2011_area)
NROW(xx_area_n)
xx_area_n <-cbind(xx_area_n, data2011_area$Operation.Type)
summary(xx_area_n)
```

### 6. Divide the data into training and testing groups 70:30
```{r}

train_index <-sample (1:nrow(xx_area_n), 0.7*nrow(yy))
train.set <- xx_area_n[train_index,]
test.set <- xx_area_n[-train_index,]
```


### 7. Create a histogram of ghg, with a limit of 0 to 100 on the x-axis and breaks of 100. 
```{r}
hist(xx_area_n$GHG.Emissions.Kg.)
#hist(xx_area_n$GHG.Emissions.Kg., xlim=c(1000,200000000), breaks= 100000)

```

### 8. Create a boxplot 
```{r}
boxplot(xx_area_n$Natural.Gas,xx_area_n$Electricity, xx_area_n$GHG.Emissions.Kg.)

```

### 9. Create a scatterplot , variance
```{r}
plot( xx_area_n$GHG.Emissions.Kg.,xx_area_n$Operation.Type)

plot(x=xx_area_n$xx_area_n$Operation.Type`, y=xx_area_n$GHG.Emissions.Kg., scale(y, center=TRUE, scale=TRUE))
plot(xx_area_n$GHG.Emissions.Kg.~xx_area_n$Electricity)
plot(xx_area_n$GHG.Emissions.Kg.~xx_area_n$Natural.Gas)
class(xx_area_n$Operation.Type)
results=aov(xx_area_n$GHG.Emissions.Kg.~ xx_area_n$Operation.Type, data=xx_area_n)
summary(results)
```

### 10. MultivariateLinear Regression model
```{r}
  model_mlr <- lm(xx_area_n$GHG.Emissions.Kg.~xx_area_n$Electricity+xx_area_n$Natural.Gas+xx_area_n$District.Heat+xx_area_n$District.Cool+xx_area_n$Fuel.Oil.1...2, data=xx_area_n) 
summary(model_mlr)
coefficients(model_mlr)
prediction <- predict(model_mlr, interval="prediction", newdata =test.set)
aov(model_mlr)
```

### 12. Look at errors and plot them on a histogram. 
```{r}
errors <- prediction[,"fit"] - test.set$GHG.Emissions.Kg.
hist(errors)

```
### 11. 
Compute the root mean square error and find the percentage of cases with less than 25% error.
```{r}
rmse <- sqrt(sum((prediction[,"fit"] - xxx_area_n$GHG.Emissions.Kg.)^2)/nrow(test.set))
rel_change <- 1 - ((test.set$GHG.Emissions.Kg. - abs(errors)) / test.set$GHG.Emissions.Kg.)
pred25 <- table(rel_change<0.25)["TRUE"] / nrow(test.set)
paste("RMSE:", rmse)
paste("PRED(25):", pred25)
```
```{r}




```

### 12. Use the forward selection algorithm. We will start with 'null', which means none of the independent variables are selected. We will come up with a selection of independent variables between 'null' and 'full'. 'full' means all the independent variables are included. Do we end up using all the variables. We set 'trace=TRUE' to see all the steps.
```{r echo=TRUE}
library(MASS)
full <- lm(xx_area$GHG.Emissions.Kg.~xx_area$Electricity+xx_area$Natural.Gas+xx_area$Fuel.Oil.1...2+xx_area$Propane+xx_area$Propane+xx_area$District.Heat,data=xx_area)
null <- lm(xx_area$GHG.Emissions.Kg.~1,data=xx_area)
stepF <- stepAIC(null, scope=list(lower=null, upper=full), direction= "forward", trace=TRUE)
#display results]
summary(stepF)
stepF$anova

```

### 13. Use nls package for non-linear regression
```{r}

#nls(formula, data, start)
#nonlin_Mod<-nls(y~data2011$Electricity, data2011, start=1000)
#nonlin_Mod<-nls(y~data2011$Electricity, data2011, start=1000)



```


```

### 16. 
```{r}

```


